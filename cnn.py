# -*- coding: utf-8 -*-
"""tp2_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zcvYq5evEY6qif_8wexYcV0AikZFyEQv
"""

!unzip donnees.zip

# Commented out IPython magic to ensure Python compatibility.
# ============================================================================
# Preparé par Mohammed Sami EL KHAIRA
# ============================================================================

# Ce modèle est un classifieur (un CNN) entrainé sur un ensemble d'images afin 
# de distinguer entre les images de dauphins et de requins.


# ============================================================================
# Données:
# ----------------------------------------------------------------------------
# entrainement : 
# classe 'dauphin': 6 500 images | classe 'requin': images 6 500 images
# validation   : 
# classe 'dauphin': 650 images | classe 'requin': images 650 images
# test         : 
# classe 'dauphin': 1 500 images | classe 'requin': images 1 500 images 
# ----------------------------------------------------------------------------


# ==========================================
# ======CHARGEMENT DES LIBRAIRIES===========
# ==========================================

# %tensorflow_version 1.x

# La libraire responsable du chargement des données dans la mémoire

from keras.preprocessing.image import ImageDataGenerator

# Le Type de notre modéle (séquentiel)

from keras.models import Model
from keras.models import Sequential

# Le type d'optimisateur utilisé dans notre modèle (RMSprop, adam, sgd, adaboost ...)
# L'optimisateur ajuste les poids de notre modèle par descente du gradient
# Chaque optimisateur a ses propres paramètres
# Note: Il faut tester plusieurs et ajuster les paramètres afin d'avoir les meilleurs résultats

from keras.optimizers import Adam, RMSprop, SGD

# Les types des couches utlilisées dans notre modèle
from keras.layers import Conv2D, MaxPooling2D, Input, BatchNormalization, \
LeakyReLU, UpSampling2D, Activation, Dropout, Flatten, Dense

# Des outils pour suivre et gérer l'entrainement de notre modèle
from keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping

# Configuration du GPU
import tensorflow as tf
from keras import backend as K

# Sauvegarde du modèle
from keras.engine.saving import load_model

# Affichage des graphes 
import matplotlib.pyplot as plt

import time

# ==========================================
# ===============GPU SETUP==================
# ==========================================

# Configuration des GPUs et CPUs
config = tf.ConfigProto(device_count={'GPU': 2, 'CPU': 4})
sess = tf.Session(config=config)
K.set_session(sess)

# ==========================================
# ================VARIABLES=================
# ==========================================

start_time = time.time()

# Le dossier principal qui contient les données
mainDataPath = "donnees/"

# Le dossier contenant les images d'entrainement
trainPath = mainDataPath + "entrainement"

# Le dossier contenant les images de validation
validationPath = mainDataPath + "validation"

# Le nom du fichier du modèle à sauvegarder
modelsPath = "Model.hdf5"

# Le nombre d'images d'entrainement et de validation
# Il faut en premier lieu identifier les paramètres du CNN qui permettent d’arriver à des bons résultats. 
# À cette fin, la démarche générale consiste à utiliser une partie des données d’entrainement et valider les résultats avec les données de validation. 
# Les paramètres du réseaux (nombre de couches de convolutions, de pooling, nombre de filtres, etc) devrait etre ajustés en conséquence. 
# Ce processus devrait se répéter jusqu’au l’obtention d’une configuration (architecture) satisfaisante. 
# Si on utilise l’ensemble de données d’entrainement en entier, le processus va être long car on devrait ajuster les paramètres et reprendre le processus sur tout l’ensemble des données d’entrainement.

training_batch_size = 11700
validation_batch_size = 1300

# Configuration des  images 
image_scale = 200
image_channels = 3
images_color_mode = "rgb" 
image_shape = (image_scale, image_scale, image_channels)

# Configuration des paramètres d'entrainement
fit_batch_size = 100
fit_epochs = 30

# ==========================================
# ==================MODÈLE==================
# ==========================================

# Couche d'entrée:
# Cette couche prend comme paramètre la forme des images (image_shape)
input_layer = Input(shape=image_shape)


# Partie feature extraction (ou cascade de couches d'extraction des caractéristiques)
def feature_extraction(input):
  
    # 1-couche de convolution avec nombre de filtre  (exp 32)  avec la taille de la fenetre de balayage exp : 3x3 
    # 2-fonction d'activation exp: sigmoid, relu, tanh ...
    # 3-couche d'echantillonage (pooling) pour reduire la taille avec la taille de la fenetre de balayage exp :2x2  
    
    # **** On répète ces étapes tant que nécessaire ****
    
    # kernel_initializer='he_uniform'
    x = Conv2D(32, (3, 3), padding='same', activation='relu')(input)
    x = Conv2D(32, (3, 3), padding='same', activation='relu')(x)
    x = MaxPooling2D((2, 2))(x)

    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)
    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)
    x = MaxPooling2D((2, 2))(x)

    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)
    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)
    x = MaxPooling2D((2, 2))(x)

    x = Conv2D(256, (3, 3), padding='same', activation='relu')(x)
    x = Conv2D(256, (3, 3), padding='same', activation='relu')(x)
    x = MaxPooling2D((2, 2))(x)

    x = Conv2D(256, (3, 3), padding='same', activation='relu')(x)
    x = Conv2D(256, (3, 3), padding='same', activation='relu')(x)
    encoded = MaxPooling2D((2, 2))(x)
    
    return encoded


# Partie complètement connectée (Fully Connected Layer)
def fully_connected(encoded):
    # Flatten: pour convertir les matrices en vecteurs pour la couche MLP
    # Dense: une couche neuronale simple avec le nombre de neurone (exemple 64)
    # fonction d'activation exp: sigmoid, relu, tanh ...
    x = Flatten(input_shape=image_shape)(encoded)

    x = Dense(256, activation='relu')(x)
    x = Dropout(0.5)(x)
    x = Dense(256, activation='relu')(x)
    x = Dropout(0.5)(x)
    x = Dense(256, activation='relu')(x)
    x = Dropout(0.5)(x)
    x = Dense(256, activation='relu')(x)
    x = Dropout(0.5)(x)

    # Puisque'on a une classification binaire, la dernière couche doit être formée d'un seul neurone avec une fonction d'activation sigmoide
    # La fonction sigmoide nous donne une valeur entre 0 et 1
    # On considère les résultats <=0.5 comme l'image appartenant à la classe 0 (c.-à-d. la classe qui correspond au dauphin)
    # on considère les résultats >0.5 comme l'image appartenant à la classe 0 (c.-à-d. la classe qui correspond au requin)
    sortie = Dense(1, activation='sigmoid')(x)
    
    return sortie


# Déclaration du modèle:
# La sortie de l'extracteur des features sert comme entrée à la couche complétement connectée
model = Model(input_layer, fully_connected(feature_extraction(input_layer)))

# Affichage des paramétres du modèle
# Cette commande affiche un tableau avec les détails du modèle 
# (nombre de couches et de paramétrer ...)
model.summary()

# Compilation du modèle :
# On définit la fonction de perte (exemple :loss='binary_crossentropy' ou loss='mse')
# L'optimisateur utilisé avec ses paramétres (Exemple : optimizer=adam(learning_rate=0.001) )
# La valeur à afficher durant l'entrainement, metrics=['accuracy'] 
# model.compile(loss='mse', optimizer='adam', metrics=['accuracy']) categorical_crossentropy binary_crossentropy
# opt = SGD(lr=0.001, momentum=0.9)
# opt = RMSprop(lr=0.0001)
opt = Adam(lr=0.0001)
# mse, mean_squared_logarithmic_error, binary_crossentropy
model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])

# ==========================================
# ==========CHARGEMENT DES IMAGES===========
# ==========================================

# training_data_generator: charge les données d'entrainement en mémoire
# quand il charge les images, il les ajuste (change la taille, les dimensions, la direction ...) 
# aléatoirement afin de rendre le modèle plus robuste à la position du sujet dans les images
# Note: On peut utiliser cette méthode pour augmenter le nombre d'images d'entrainement (data augmentation)
training_data_generator = ImageDataGenerator(
    rescale=1. / 255,
    shear_range=0.1,
    zoom_range=0.1,
    horizontal_flip=True)

    # shear_range=0.1,
    # zoom_range=0.1,
    # horizontal_flip=True
    # width_shift_range=0.1, 
    # height_shift_range=0.1,
    # rotation_range=0.1,
    # vertical_flip=True,

# validation_data_generator: charge les données de validation en memoire
validation_data_generator = ImageDataGenerator(rescale=1. / 255)

# training_generator: indique la méthode de chargement des données d'entrainement
training_generator = training_data_generator.flow_from_directory(
    trainPath, # Place des images d'entrainement
    color_mode=images_color_mode, # couleur des images
    target_size=(image_scale, image_scale),# taille des images
    batch_size=training_batch_size, # nombre d'images à entrainer (batch size)
    class_mode="binary", # classement binaire (problème de 2 classes)
    shuffle=True) # on "brasse" (shuffle) les données -> pour prévenir le surapprentissage

# validation_generator: indique la méthode de chargement des données de validation
validation_generator = validation_data_generator.flow_from_directory(
    validationPath, # Place des images de validation
    color_mode=images_color_mode, # couleur des images
    target_size=(image_scale, image_scale),  # taille des images
    batch_size=validation_batch_size,  # nombre d'images à valider
    class_mode="binary",  # classement binaire (problème de 2 classes)
    shuffle=True) # on "brasse" (shuffle) les données -> pour prévenir le surapprentissage

# On imprime l'indice de chaque classe (Keras numerote les classes selon l'ordre des dossiers des classes)
# Dans ce cas => [dauphin: 0 et requin:1]
print(training_generator.class_indices)
print(validation_generator.class_indices)

# On charge les données d'entrainement et de validation
# x_train: Les données d'entrainement
# y_train: Les étiquettes des données d'entrainement
# x_val: Les données de validation
# y_val: Les étiquettes des données de validation
(x_train, y_train) = training_generator.next()
(x_val, y_val) = validation_generator.next()

# On Normalise les images en les divisant par la plus grande pixel dans les images (generalement c'est 255)
# Alors on aura des valeur entre 0 et 1, ceci stabilise l'entrainement
max_value = float(x_train.max())
x_train = x_train.astype('float32') / max_value
x_val = x_val.astype('float32') / max_value

# ==========================================
# ==============ENTRAINEMENT================
# ==========================================

# Savegarder le modèle avec la meilleure validation accuracy ('val_acc') 
# Note: on sauvegarder le modèle seulement quand la précision de la validation s'améliore
modelcheckpoint = ModelCheckpoint(filepath=modelsPath,
                                  monitor='val_acc', verbose=1, save_best_only=True, mode='auto')

# entrainement du modèle
classifier = model.fit(x_train, y_train,
                       epochs=fit_epochs, # nombre d'époques
                       batch_size=fit_batch_size, # nombre d'images entrainées ensemble
                       validation_data=(x_val, y_val), # données de validation
                       verbose=1, # mets cette valeur ‡ 0, si vous voulez ne pas afficher les détails d'entrainement
                       callbacks=[modelcheckpoint], # les fonctions à appeler à la fin de chaque époque (dans ce cas modelcheckpoint: qui sauvegarde le modèle)
                       shuffle=True)# shuffle les images 

# ==========================================
# ========AFFICHAGE DES RESULTATS===========
# ==========================================

# Plot accuracy over epochs (precision par époque)
print(classifier.history.keys())
plt.plot(classifier.history['acc'])
plt.plot(classifier.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'])
fig = plt.gcf()
plt.show()


# Plot loss over epochs (perte par époque)
print(classifier.history.keys())
plt.plot(classifier.history['loss'])
plt.plot(classifier.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'])
fig = plt.gcf()
plt.show()

# Execution time :
minutes = int((time.time() - start_time) / 60)
seconds = int((time.time() - start_time) % 60)
print("Total execution time: {0} minutes and {1} seconds".format(minutes, seconds))

# ============================================================================
# Preparé par Mohammed Sami EL KHAIRA
# ============================================================================

# Dans ce script, on évalue le modèle entrainé précédemment.
# On charge le modèle en mémoire, on charge les images et puis on applique 
# le modèle sur les images afin de prédire les classes


# ==========================================
# ======CHARGEMENT DES LIBRAIRIES===========
# ==========================================

# La libraire responsable du chargement des données dans la mémoire
from keras.preprocessing.image import ImageDataGenerator

# Affichage des graphes
import matplotlib.pyplot as plt
import matplotlib.image as mpimg

# La librairie numpy 
import numpy as np

# Configuration du GPU
import tensorflow as tf
from keras import backend as K

# Utilisé pour le calcul des métriques de validation
from sklearn.metrics import confusion_matrix, roc_curve , roc_auc_score

# Utlilisé pour charger le modèle
from keras.models import load_model
from keras import Model


# ==========================================
# ===============GPU SETUP==================
# ==========================================

# Configuration des GPUs et CPUs
config = tf.ConfigProto(device_count={'GPU': 2, 'CPU': 4})
sess = tf.Session(config=config)
K.set_session(sess)

# ==========================================
# ==================MODÈLE==================
# ==========================================

#Chargement du modéle sauvegardé dans la section 1 via 1_Modele.py
model_path = "Model.hdf5"
Classifier: Model = load_model(model_path)

# ==========================================
# ================VARIABLES=================
# ==========================================


# L'emplacement des images de test
mainDataPath = "donnees/"
testPath = mainDataPath + "test"

# Le nombre des images de test à évaluer
number_images = 3000
number_images_class_0 = 1500
number_images_class_1 = 1500

# La taille des images à classer
image_scale = 200

# La couleur des images à classer
images_color_mode = "rgb"

# ==========================================
# =========CHARGEMENT DES IMAGES============
# ==========================================

# Chargement des images de test
test_data_generator = ImageDataGenerator(rescale=1. / 255)

test_itr = test_data_generator.flow_from_directory(
    testPath,# place des images
    target_size=(image_scale, image_scale), # taille des images
    class_mode="binary",# Type de classification
    shuffle=False,# pas besoin de les boulverser
    batch_size=1,# on classe les images une à la fois
    color_mode=images_color_mode)# couleur des images

(x, y_true) = test_itr.next()

# Normalize Data
max_value = float(x.max())
x = x.astype('float32') / max_value

# ==========================================
# ===============ÉVALUATION=================
# ==========================================

# Les classes correctes des images (1500 pour chaque classe) -- the ground truth
y_true = np.array([0] * number_images_class_0 + 
                  [1] * number_images_class_1)

# Évaluation du modèle
test_eval = Classifier.evaluate_generator(test_itr, verbose=1)

# Affichage des valeurs de perte et de precision
print('>Test loss (Erreur):', test_eval[0])
print('>Test précision:', test_eval[1])

# Prédiction des classes des images de test
predicted_classes = Classifier.predict_generator(test_itr, verbose=1)
predicted_classes_perc = np.round(predicted_classes.copy(), 4)
predicted_classes = np.round(predicted_classes) # on arrondie le output
# 0 => classe dauphin
# 1 => classe requin

# Cette list contient les images bien classées
correct = []
for i in range(0, len(predicted_classes) - 1):
    if predicted_classes[i] == y_true[i]:
        correct.append(i)
# Nombre d'images bien classées
print("> %d  Étiquettes bien classées" % len(correct))

# Cette list contient les images mal classées
incorrect = []
for i in range(0, len(predicted_classes) - 1):
    if predicted_classes[i] != y_true[i]:
        incorrect.append(i)
# Nombre d'images mal classées
print("> %d Étiquettes mal classées" % len(incorrect))


def print_roc_curve(y_test, y_pred, title):
    false_positive_rate, true_positive_rate, threshold = roc_curve(y_test, y_pred)
    auc = str(np.round(roc_auc_score(y_test, y_pred), decimals=4))
    plt.title(title)
    plt.plot(false_positive_rate, true_positive_rate, label="AUC = {0}".format(auc))
    plt.plot([0, 1], ls="--")
    plt.plot([0, 0], [0, 1], c="0.7")
    plt.plot([0, 1], [1, 1], c="0.7")
    plt.ylabel("True Positive Rate")
    plt.xlabel("False Positive Rate")
    plt.legend(loc="lower right")
    plt.show()

print("\nMatrice de confusion")
print(confusion_matrix(y_true, predicted_classes))
print_roc_curve(y_true, predicted_classes_perc, "CNN ROC Curve")

tp = 0; fp = 0; fn = 0; tn = 0
for i, pred_class in enumerate(predicted_classes):
  if pred_class == y_true[i]:
    if pred_class == 0 and tp < 5:
      print("Dauphin bien classé")
      tp = tp + 1
      image_path = testPath + "/" + test_itr.filenames[i]
      img = mpimg.imread(image_path, 0)
      plt.imshow(img)
      plt.show()
    elif pred_class == 1 and tn < 5:
      print("Requin bien classé")
      tn = tn + 1
      image_path = testPath + "/" + test_itr.filenames[i]
      img = mpimg.imread(image_path, 0)
      plt.imshow(img)
      plt.show()
  else:
    if pred_class == 0 and fp < 5:
      print("Requin classé comme dauphin")
      fp = fp + 1
      image_path = testPath + "/" + test_itr.filenames[i]
      img = mpimg.imread(image_path, 0)
      plt.imshow(img)
      plt.show()
    elif pred_class == 1 and fn < 5:
      print("Dauphin classé comme requin")
      fn = fn + 1
      image_path = testPath + "/" + test_itr.filenames[i]
      img = mpimg.imread(image_path, 0)
      plt.imshow(img)
      plt.show()
  if tp == 5 and fp == 5 and fn == 5 and tn == 5:
    break